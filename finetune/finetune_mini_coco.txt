Script started on 2025-11-26 09:08:33+00:00 [TERM="screen" TTY="/dev/pts/1" COLUMNS="135" LINES="35"]
 ______                 ______            _
(_____ \               (_____ \          | |
 _____) ) _   _  ____   _____) )___    __| |
|  __  / | | | ||  _ \ |  ____// _ \  / _  |
| |  \ \ | |_| || | | || |    | |_| |( (_| |
|_|   |_||____/ |_| |_||_|     \___/  \____|

For detailed documentation and guides, please visit:
[1;34mhttps://docs.runpod.io/[0m and [1;34mhttps://blog.runpod.io/[0m


[?2004hroot@e4e62b9d78b7:/workspace/detr# python finetune/finetune_mini_coco.py 
[?2004lUsing device: cuda
loading annotations into memory...
Done (t=0.57s)
creating index...
index created!
Total images: 5000
Train images: 4000
Val images:   1000
/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Loading checkpoint from /workspace/detr/detr-r50-e632da11.pth ...
/workspace/detr/finetune/finetune_mini_coco.py:157: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  ckpt = torch.load(ckpt_path, map_location="cpu")
Checkpoint loaded.
Epoch [1/5] Iter [50/2000] Loss: 144.4845
Epoch [1/5] Iter [100/2000] Loss: 142.8269
Epoch [1/5] Iter [150/2000] Loss: 164.9692
Epoch [1/5] Iter [200/2000] Loss: 142.5360
Epoch [1/5] Iter [250/2000] Loss: 136.5292
Epoch [1/5] Iter [300/2000] Loss: 127.9208
Epoch [1/5] Iter [350/2000] Loss: 132.0697
Epoch [1/5] Iter [400/2000] Loss: 115.9949
Epoch [1/5] Iter [450/2000] Loss: 137.0854
Epoch [1/5] Iter [500/2000] Loss: 102.5609
Epoch [1/5] Iter [550/2000] Loss: 92.7949
Epoch [1/5] Iter [600/2000] Loss: 101.5086
Epoch [1/5] Iter [650/2000] Loss: 87.8614
Epoch [1/5] Iter [700/2000] Loss: 93.9749
Epoch [1/5] Iter [750/2000] Loss: 92.3245
Epoch [1/5] Iter [800/2000] Loss: 91.3534
Epoch [1/5] Iter [850/2000] Loss: 78.2387
Epoch [1/5] Iter [900/2000] Loss: 80.3661
Epoch [1/5] Iter [950/2000] Loss: 90.8636
Epoch [1/5] Iter [1000/2000] Loss: 79.8972
Epoch [1/5] Iter [1050/2000] Loss: 66.1790
Epoch [1/5] Iter [1100/2000] Loss: 83.0335
Epoch [1/5] Iter [1150/2000] Loss: 72.9166
Epoch [1/5] Iter [1200/2000] Loss: 82.9898
Epoch [1/5] Iter [1250/2000] Loss: 83.9655
Epoch [1/5] Iter [1300/2000] Loss: 93.6083
Epoch [1/5] Iter [1350/2000] Loss: 87.2835
Epoch [1/5] Iter [1400/2000] Loss: 88.8384
Epoch [1/5] Iter [1450/2000] Loss: 93.8626
Epoch [1/5] Iter [1500/2000] Loss: 80.8854
Epoch [1/5] Iter [1550/2000] Loss: 88.3903
Epoch [1/5] Iter [1600/2000] Loss: 77.9767
Epoch [1/5] Iter [1650/2000] Loss: 90.5721
Epoch [1/5] Iter [1700/2000] Loss: 100.1404
Epoch [1/5] Iter [1750/2000] Loss: 75.3905
Epoch [1/5] Iter [1800/2000] Loss: 100.2465
Epoch [1/5] Iter [1850/2000] Loss: 93.9778
Epoch [1/5] Iter [1900/2000] Loss: 74.8038
Epoch [1/5] Iter [1950/2000] Loss: 78.9998
Epoch [1/5] Iter [2000/2000] Loss: 85.0662
Epoch 1: val loss = 76.5753
Saved checkpoint: /workspace/detr/detr_r50_mini_finetune_epoch1.pth
Epoch [2/5] Iter [50/2000] Loss: 80.6351
Epoch [2/5] Iter [100/2000] Loss: 81.0106
Epoch [2/5] Iter [150/2000] Loss: 71.3778
Epoch [2/5] Iter [200/2000] Loss: 78.4129
Epoch [2/5] Iter [250/2000] Loss: 71.5301
Epoch [2/5] Iter [300/2000] Loss: 69.3270
Epoch [2/5] Iter [350/2000] Loss: 68.7091
Epoch [2/5] Iter [400/2000] Loss: 75.8254
Epoch [2/5] Iter [450/2000] Loss: 71.5087
Epoch [2/5] Iter [500/2000] Loss: 80.7079
Epoch [2/5] Iter [550/2000] Loss: 79.4419
Epoch [2/5] Iter [600/2000] Loss: 79.2044
Epoch [2/5] Iter [650/2000] Loss: 63.3532
Epoch [2/5] Iter [700/2000] Loss: 88.3307
Epoch [2/5] Iter [750/2000] Loss: 80.6445
Epoch [2/5] Iter [800/2000] Loss: 77.0699
Epoch [2/5] Iter [850/2000] Loss: 76.5980
Epoch [2/5] Iter [900/2000] Loss: 76.1466
Epoch [2/5] Iter [950/2000] Loss: 81.1788
Epoch [2/5] Iter [1000/2000] Loss: 80.3151
Epoch [2/5] Iter [1050/2000] Loss: 76.6624
Epoch [2/5] Iter [1100/2000] Loss: 81.8233
Epoch [2/5] Iter [1150/2000] Loss: 72.6386
Epoch [2/5] Iter [1200/2000] Loss: 93.4731
Epoch [2/5] Iter [1250/2000] Loss: 76.9870
Epoch [2/5] Iter [1300/2000] Loss: 76.1608
Epoch [2/5] Iter [1350/2000] Loss: 66.2435
Epoch [2/5] Iter [1400/2000] Loss: 79.7406
Epoch [2/5] Iter [1450/2000] Loss: 87.0352
Epoch [2/5] Iter [1500/2000] Loss: 86.1529
Epoch [2/5] Iter [1550/2000] Loss: 79.3601
Epoch [2/5] Iter [1600/2000] Loss: 82.3286
Epoch [2/5] Iter [1650/2000] Loss: 70.1579
Epoch [2/5] Iter [1700/2000] Loss: 90.2811
Epoch [2/5] Iter [1750/2000] Loss: 92.6171
Epoch [2/5] Iter [1800/2000] Loss: 77.5914
Epoch [2/5] Iter [1850/2000] Loss: 78.3523
Epoch [2/5] Iter [1900/2000] Loss: 96.1397
Epoch [2/5] Iter [1950/2000] Loss: 97.5386
Epoch [2/5] Iter [2000/2000] Loss: 80.3041
Epoch 2: val loss = 71.3159
Saved checkpoint: /workspace/detr/detr_r50_mini_finetune_epoch2.pth
Epoch [3/5] Iter [50/2000] Loss: 83.9233
Epoch [3/5] Iter [100/2000] Loss: 76.1817
Epoch [3/5] Iter [150/2000] Loss: 67.1937
Epoch [3/5] Iter [200/2000] Loss: 77.3182
Epoch [3/5] Iter [250/2000] Loss: 71.0138
Epoch [3/5] Iter [300/2000] Loss: 58.3804
Epoch [3/5] Iter [350/2000] Loss: 75.2557
Epoch [3/5] Iter [400/2000] Loss: 59.7824
Epoch [3/5] Iter [450/2000] Loss: 93.1486
Epoch [3/5] Iter [500/2000] Loss: 82.9698
Epoch [3/5] Iter [550/2000] Loss: 79.0238
Epoch [3/5] Iter [600/2000] Loss: 84.3692
Epoch [3/5] Iter [650/2000] Loss: 89.3085
Epoch [3/5] Iter [700/2000] Loss: 79.4653
Epoch [3/5] Iter [750/2000] Loss: 68.5163
Epoch [3/5] Iter [800/2000] Loss: 78.9927
Epoch [3/5] Iter [850/2000] Loss: 91.1939
Epoch [3/5] Iter [900/2000] Loss: 91.4384
Epoch [3/5] Iter [950/2000] Loss: 80.1268
Epoch [3/5] Iter [1000/2000] Loss: 83.6928
Epoch [3/5] Iter [1050/2000] Loss: 77.5398
Epoch [3/5] Iter [1100/2000] Loss: 85.7049
Epoch [3/5] Iter [1150/2000] Loss: 76.1430
Epoch [3/5] Iter [1200/2000] Loss: 82.4839
Epoch [3/5] Iter [1250/2000] Loss: 97.2993
Epoch [3/5] Iter [1300/2000] Loss: 83.2855
Epoch [3/5] Iter [1350/2000] Loss: 81.4280
Epoch [3/5] Iter [1400/2000] Loss: 81.4081
Epoch [3/5] Iter [1450/2000] Loss: 76.1932
Epoch [3/5] Iter [1500/2000] Loss: 71.6899
Epoch [3/5] Iter [1550/2000] Loss: 84.9417
Epoch [3/5] Iter [1600/2000] Loss: 79.1820
Epoch [3/5] Iter [1650/2000] Loss: 74.7790
Epoch [3/5] Iter [1700/2000] Loss: 85.1162
Epoch [3/5] Iter [1750/2000] Loss: 87.1477
Epoch [3/5] Iter [1800/2000] Loss: 74.1239
Epoch [3/5] Iter [1850/2000] Loss: 74.7542
Epoch [3/5] Iter [1900/2000] Loss: 71.8735
Epoch [3/5] Iter [1950/2000] Loss: 69.3776
Epoch [3/5] Iter [2000/2000] Loss: 74.6174
Epoch 3: val loss = 80.8862
Saved checkpoint: /workspace/detr/detr_r50_mini_finetune_epoch3.pth
Epoch [4/5] Iter [50/2000] Loss: 80.0122
Epoch [4/5] Iter [100/2000] Loss: 77.2578
Epoch [4/5] Iter [150/2000] Loss: 75.5248
Epoch [4/5] Iter [200/2000] Loss: 82.9567
Epoch [4/5] Iter [250/2000] Loss: 82.0247
Epoch [4/5] Iter [300/2000] Loss: 76.9897
Epoch [4/5] Iter [350/2000] Loss: 71.7727
Epoch [4/5] Iter [400/2000] Loss: 69.3627
Epoch [4/5] Iter [450/2000] Loss: 65.3790
Epoch [4/5] Iter [500/2000] Loss: 73.8596
Epoch [4/5] Iter [550/2000] Loss: 78.9816
Epoch [4/5] Iter [600/2000] Loss: 79.2996
Epoch [4/5] Iter [650/2000] Loss: 65.6067
Epoch [4/5] Iter [700/2000] Loss: 75.1706
Epoch [4/5] Iter [750/2000] Loss: 76.2777
Epoch [4/5] Iter [800/2000] Loss: 72.0637
Epoch [4/5] Iter [850/2000] Loss: 77.7094
Epoch [4/5] Iter [900/2000] Loss: 77.2498
Epoch [4/5] Iter [950/2000] Loss: 75.0688
Epoch [4/5] Iter [1000/2000] Loss: 70.8996
Epoch [4/5] Iter [1050/2000] Loss: 76.5250
Epoch [4/5] Iter [1100/2000] Loss: 72.5217
Epoch [4/5] Iter [1150/2000] Loss: 84.2601
Epoch [4/5] Iter [1200/2000] Loss: 81.6143
Epoch [4/5] Iter [1250/2000] Loss: 70.6589
Epoch [4/5] Iter [1300/2000] Loss: 68.3408
Epoch [4/5] Iter [1350/2000] Loss: 73.3640
Epoch [4/5] Iter [1400/2000] Loss: 73.5988
Epoch [4/5] Iter [1450/2000] Loss: 77.5538
Epoch [4/5] Iter [1500/2000] Loss: 72.1028
Epoch [4/5] Iter [1550/2000] Loss: 92.5880
Epoch [4/5] Iter [1600/2000] Loss: 73.8347
Epoch [4/5] Iter [1650/2000] Loss: 79.0148
Epoch [4/5] Iter [1700/2000] Loss: 74.1913
Epoch [4/5] Iter [1750/2000] Loss: 75.0371
Epoch [4/5] Iter [1800/2000] Loss: 71.4011
Epoch [4/5] Iter [1850/2000] Loss: 97.2880
Epoch [4/5] Iter [1900/2000] Loss: 77.2198
Epoch [4/5] Iter [1950/2000] Loss: 76.3224
Epoch [4/5] Iter [2000/2000] Loss: 82.8778
Epoch 4: val loss = 88.1026
Saved checkpoint: /workspace/detr/detr_r50_mini_finetune_epoch4.pth
Epoch [5/5] Iter [50/2000] Loss: 79.7325
Epoch [5/5] Iter [100/2000] Loss: 74.7932
Epoch [5/5] Iter [150/2000] Loss: 83.6494
Epoch [5/5] Iter [200/2000] Loss: 77.3406
Epoch [5/5] Iter [250/2000] Loss: 91.0394
Epoch [5/5] Iter [300/2000] Loss: 74.0707
Epoch [5/5] Iter [350/2000] Loss: 83.3209
Epoch [5/5] Iter [400/2000] Loss: 66.6252
Epoch [5/5] Iter [450/2000] Loss: 84.5643
Epoch [5/5] Iter [500/2000] Loss: 87.2537
Epoch [5/5] Iter [550/2000] Loss: 78.7776
Epoch [5/5] Iter [600/2000] Loss: 71.9517
Epoch [5/5] Iter [650/2000] Loss: 77.8849
Epoch [5/5] Iter [700/2000] Loss: 72.6510
Epoch [5/5] Iter [750/2000] Loss: 77.6313
Epoch [5/5] Iter [800/2000] Loss: 72.6831
Epoch [5/5] Iter [850/2000] Loss: 84.4798
Epoch [5/5] Iter [900/2000] Loss: 86.6882
Epoch [5/5] Iter [950/2000] Loss: 66.3040
Epoch [5/5] Iter [1000/2000] Loss: 63.3023
Epoch [5/5] Iter [1050/2000] Loss: 74.3816
Epoch [5/5] Iter [1100/2000] Loss: 85.5418
Epoch [5/5] Iter [1150/2000] Loss: 79.5682
Epoch [5/5] Iter [1200/2000] Loss: 81.3824
Epoch [5/5] Iter [1250/2000] Loss: 62.5440
Epoch [5/5] Iter [1300/2000] Loss: 87.2749
Epoch [5/5] Iter [1350/2000] Loss: 70.0170
Epoch [5/5] Iter [1400/2000] Loss: 73.9697
Epoch [5/5] Iter [1450/2000] Loss: 77.2356
Epoch [5/5] Iter [1500/2000] Loss: 79.5601
Epoch [5/5] Iter [1550/2000] Loss: 57.6599
Epoch [5/5] Iter [1600/2000] Loss: 84.7580
Epoch [5/5] Iter [1650/2000] Loss: 76.2456
Epoch [5/5] Iter [1700/2000] Loss: 71.7473
Epoch [5/5] Iter [1750/2000] Loss: 75.2339
Epoch [5/5] Iter [1800/2000] Loss: 89.3338
Epoch [5/5] Iter [1850/2000] Loss: 86.2914
Epoch [5/5] Iter [1900/2000] Loss: 66.9783
Epoch [5/5] Iter [1950/2000] Loss: 79.4172
Epoch [5/5] Iter [2000/2000] Loss: 74.8749
Epoch 5: val loss = 76.0612
Saved checkpoint: /workspace/detr/detr_r50_mini_finetune_epoch5.pth
[?2004hroot@e4e62b9d78b7:/workspace/detr# exit
[?2004lexit

Script done on 2025-11-26 09:28:50+00:00 [COMMAND_EXIT_CODE="0"]
